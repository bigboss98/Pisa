\chapter{Linear Regression}
Linear Regression is in statistics a linear process to modeling the relation between
a scalar response (or dependent variable) and one or more explanatory variables (or independent variables).

The case of one explanatory variable is called \emph{simple linear regression} and 
for more than one explanatory variable, the process is called \emph{multiple linear regression}.

Suppose for example we want to predict the price of an house, given living area and the number of bedrooms,
so we represent this relation as a linear regression with the hypothesis function as 
\[ h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_1 x_2 \]
where $x_1$ is the living area and $x_2$ is the number of bedrooms.

Here, the $\theta_i$’s are the parameters(also called \emph{weights}) parameterizing the 
space of linear functions mapping from $x$ to $y$ and when there is no risk of confusion,
we will drop the $\theta$ subscript in $h_{\theta}(x)$ and write it more simply as $h(x)$.

To simplify our notation, we also introduce the convention of letting $x_0 = 1$
(this is the intercept term), so we have 
\[ h_{\theta}(x) = \sum _{i = 0} ^n \theta_i x_i = \transpose{\theta} x \]
where on the right-hand side above we are viewing $\theta$ and $x$ both as vectors, 
and here $n$ is the number of input variables, without counting $x_0$.

\section{Least squares cost function}
Given the training set, to learn $\theta$ parameters a reasonable choice is to make
$h(x)$ close to $y$, at least for the training examples we have.

To formalize this, we will define a function that measures, for each value of the $\theta$’s,
how close the $h(x^{(i)})$’s are to the corresponding $y^{(i)}$’s.

We define the cost function as
\[ J(\theta) = \frac{1}{2} \sum _{i=0} ^n \left (h_{\theta}(x^{(i)}) - y^{(i)} \right )^2 \]
where we divide by $\frac{1}{2}$ in order to simply derivation in future.

If you’ve seen linear regression before, you may recognize this as the familiar 
\emph{least-squares} cost function that gives rise to the ordinary least squares regression model.\newline
Whether or not you have seen it previously, let’s keep going, and we’ll eventually show this to be
a special case of a much broader family of algorithms.

Our objective is to find $\theta$ that minimize $J(\theta)$ so let’s consider now
the \emph{gradient descent} algorithm, which starts with some initial $\theta$, and repeatedly performs the update:
\[ \theta_j = \theta_j - \alpha * \derivative{\theta_j} J(\theta) \]
This update is simultaneously performed for all values of $j= 0, 1, \dots, n$ and 
$\alpha$ is called the \emph{learning rate}.

This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of J
and in order to implement this algorithm, we have to work out what is 
the partial derivative term on the right hand side.\newline
Let’s first work it out for the case we have only one training example $(x, y)$, so that we
can neglect the sum in the definition of J and we have 

\begin{align*}
    \derivative{\theta_j} = & \derivative{\theta_j} \frac{1}{2} (h_{\theta}(x) - y)^2 \\
                          = & (h_{\theta}(x) - y) \derivative{\theta_j} (h_{\theta}(x)-y) \\
                          = & (h_{\theta}(x) - y) \derivative{\theta_j} (\sum _{i = 0}^n \theta_i x_i - y) \\
                          = & (h_{\theta}(x) - y)x_j \\
\end{align*}
For a simple training example, it provides the update rule as
\[ \theta_j = \theta_j - \alpha (h_{\theta}(x) - y)x_j ^{(i)} \]
The rule is called the \emph{LMS update rule}(LMS stands for “least mean squares”), and is also known
as the \emph{Widrow-Hoff} learning rule.\newline
This rule has several properties that seem natural and intuitive in fact for instance, the magnitude of 
    the update is proportional to the error term $(y(i) - h_{\theta}(x^{(i)}))$; 
thus, for instance, if we are encountering a training example on which our prediction nearly matches
the actual value of $y^{(i)}$, then we find that there is little need to change the parameters,
in contrast, a larger change to the parameters will be made if our prediction $h_{\theta}(x^{(i)})$ 
has a large error.

We’d derived the LMS rule for when there was only a single training example and 
to modify this method for a training set of more than one example,
we replace the LMS rule with the following algorithm, where we repeat until the convergence the update rule
\[ \theta_j = \theta_j - \alpha (\sum _{i=0}^m (y^{(i)} - h_{\theta}(x^{(i)}) x_j^{(i)} 
   \quad \forall j = 1, 2, \dots, m \]
This is simply gradient descent on the original cost function $J$ and this method looks at every example
in the entire training set on every step, and it is called \emph{batch gradient descent}.\newline
Note that, while gradient descent can be susceptible to local minima in general, the optimization problem
we have posed here for linear regression has only one global, and no other local optima, because 
$J$ is a convex quadratic function.

Another algorithm to train the $\theta$ parameters is \emph{stochastic descent}, where 
we repeatedly run through the training set, and each time we encounter a training example,
we update the parameters according to the gradient of the error with respect to that single training example only.

Whereas batch gradient descent has to scan through the entire training set before taking a single step,
a costly operation if $m$ is large, stochastic gradient descent can start making progress right away,
and continues to make progress with each example it looks at.\newline
Often, stochastic gradient descent gets $\theta$ "close” to the minimum much faster than batch 
gradient descent, but note however that it may never “converge” to the minimum, and the parameters
$\theta$ will keep oscillating around the minimum of $J(\theta)$; in practice most of the values near the minimum
will be reasonably good approximations to the true minimum and for these reasons, particularly when 
the training set is large, stochastic gradient descent is often preferred over batch gradient descent.

\section{Normal Equation}
Gradient Descent gives a way to minimize $J(\theta)$, we present now another method, where 
we performing the minimization explicitly and without resorting to an iterative algorithm.\newline
In this method, we will minimize $J$ by explicitly taking its derivatives with respect to the $\theta_j$’s,
and setting them to zero.\newline
So we have
\begin{align*}
	\myDeriv{J(\theta)}{\theta_j} & = \myDeriv{\frac{1}{2} \sum_{i=1}^m (y^{(i)}-\transpose{\theta}x^{(i)})^2}{\theta_j} \\
	                                 & = \sum _{i=1}^m (y^{(i)} - \transpose{\theta}x^{(i)}) \myDeriv{(y^{(i)} - \transpose{\theta}x^{(i)})}{\theta_j} \\
					 & = - \sum _{i=1}^m (y^{(i)} - \transpose{\theta}x^{(i)}) x^{(i)}_j \\
\end{align*}
To minimize $J$, we set its derivatives to zero, and obtain the normal equations
\[ \transpose{X} X \theta = \transpose{X} y \]
so the value of $\theta$ that minimize $J(\theta)$ is given by 
\[ \theta = (\transpose{X} X)^{-1} \transpose{X} y = X^+ y\]
where $X^+$ is the Moore-Penrose pseudoinverse that is compute also when $X$ is not invertible.\newline
Singular Value Decomposition can be used for computing the pseudoinverse of a matrix, infact we have $X = U \Sigma \transpose{V}$ that imply 
\[ X^+ = V \Sigma^+ \transpose{U} \]
so we can use SVD to compute $\theta = X^+ y$ (more details can be found on Computation Math course).

\section{Probabilistic Interpretation of Linear Regression}
In this section, we will give a set of probabilistic assumptions, under which least-squares regression
is derived as a very natural algorithm and let us assume that the target variables and the inputs 
are related via the equation
\[ y^{(i)} = \transpose{\theta} x + \epsilon ^{(i)} \]
where $\epsilon{(i)}$ is an error term that captures either unmodeled effects, such as if there are some features
very pertinent to predicting housing price, but that we’d left out of the regression, or random noise.

Let us further assume that the $\epsilon^{(i)}$ are distributed IID (independently and identically distributed)
according to a Gaussian distribution as $\epsilon^{(i)} \sim N(0, \sigma^2)$, that implies that 
$P(y^{(i)} | x^{(i)} ; \theta) \sim N(\transpose{\theta}x^{(i)}, \sigma^2)$.

Given $X$, the design matrix, which contains all the $x^{(i)}$’s, and $\theta$, 
the probability of the data is given by $P(y| X; \theta)$ and this quantity is typically viewed 
as a function of $y$, and perhaps $X$, for a fixed value of $\theta$.\newline
When we wish to explicitly view this as a function of $\theta$, we will instead call it the likelihood function
\begin{align*}
    L(\theta) & = L(\theta | X; y) = P(y | X; \theta) \\
              & = \prod _{i = 1} ^ m P(y^{(i)} | x^{(i)}; \theta) \\
              & = \prod _{i = 1} ^ m \gaussian{\transpose{\theta}x^{(i)}}{\sigma ^ 2} \\
\end{align*}

Now, given this probabilistic model relating the $y^{(i)}$’s and the $x^{(i)}$’s, a reasonable way
of choosing our best guess of the parameters $\theta$, we should choose $\theta$ to maximize $L(\theta)$.

Instead of maximizing $L(\theta)$, we can also maximize any strictly increasing function of $L(\theta)$, 
in particular, the derivations will be a bit simpler if we  instead maximize the log likelihood $l(\theta)$
\begin{align*}
    l(\theta) & = \log L(\theta) \\
              & = \log \prod _{i=1} ^ m \gaussian{\transpose{\theta}y^{(i)}}{\sigma ^ 2} \\
              & = \sum _{i = 1} ^ m \log \gaussian{\transpose{\theta}y^{(i)}}{\sigma^2} \\
              & = m \log \frac{1}{\sqrt{2 \pi} \sigma} - 
                  \frac{1}{\sigma^2} \frac{1}{2} \sum _{i=1}^m (y^{(i)} - \transpose{\theta}x^{(i)})^2 \\
\end{align*}
Here maximizing $l(\theta)$ gives the same answer that minimizing
\[ \frac{1}{2} \sum _{i = 1} (y^{(i)} - \transpose{\theta}x^{(i)})^2 \]
which we recognise to be $J(\theta)$, our original least-squares cost function.

Under the previous probabilistic assumptions on the data, least-squares regression corresponds
to finding the maximum likelihood estimate of $\theta$ and this is thus one set of assumptions 
under which least-squares regression can be justified as a very natural method that’s just 
doing maximum likelihood estimation.

Note however that the probabilistic assumptions are by no means necessary for least-squares to be 
a perfectly good and rational procedure, and there may, and indeed there are, other natural assumptions
that can also be used to justify it.

\section{Newton optimization to maximise $l(\theta)$}
Specifically, suppose we have some function $f:\R \to \R$, and we wish to find a value of $\theta$
so that $f(\theta) = 0$.\newline
Here, $\theta \in \R$ is a real number and Newton’s method performs the following update
\[ \theta = \theta - \frac{f(\theta)}{f'(\theta)} \]
This method has a natural interpretation in which we can think of it as approximating the function $f$
via a linear function that is tangent to $f$ at the current guess $\theta$, solving for where that 
linear function equals to zero, and letting the next guess for $\theta$ be where that linear function is zero.

Newton’s method gives a way of getting to $f(\theta) = 0$ and to maximise $l(\theta)$ we have to consider that
the maxima of $l$ correspond to points where its first derivative $l'(\theta)$ is zero.\newline
So, by letting $f(\theta) = l'(\theta)$, we can use the same algorithm to maximize $l$, and we obtain the update rule
\[ \theta = \theta - \frac{l'(\theta)}{l''(\theta)} \]
The generalization of Newton’s method to multidimensional setting(also called the \emph{Newton-Raphson} method)
is given by 
\[ \theta = \theta - H^{-1} \Delta _{\theta} \, l(\theta) \]
where $H^{-1}$ is the second derivative called \emph{Hessian}.

Newton’s method typically enjoys faster convergence than (batch) gradient descent, and requires
many fewer iterations to get very close to the minimum.\newline
One iteration of Newton’s can, however, be more expensive than one iteration of gradient descent,
since it requires finding and inverting an $n \times n$ Hessian; but so long as $n$ is not too large,
it is usually much faster over all.\newline
When Newton’s method is applied to maximize the logistic regression log likelihood function $l(\theta)$,
the resulting method is also called \emph{Fisher scoring}.


\section{Control model complexity}
To control the model complexity there are several approaches, like do a subset reduction, coefficient shrinkage, but we will now introduce and 
concentrate on \emph{Regularization}, that consist to keep track that an higher complexity VC correspond overfittig, so we define now the \emph{Ridge regression} as 
\begin{defi}
	Ridge regression shrinks the regression coefficients by imposing a penalty on their size and ridge coefficients minimize a penalized residual sum of squares
	\[ \hat{\theta_{ridge}} = argmin \{ \sum _{i=1}^N (y_i - \sum _{j=0}^P x_{ij} \theta_j)^2 + \lambda \sum _{j=0}^P \theta_j^2 \} \]
\end{defi}
$\lambda \geq 0$ control the amount of shrinkage and the idea of penalizing is also used in NN, where it is known as \emph{weight decay} and with Ridge regularization 
we have the loss function defined as 
\[ J(\theta) = \frac{1}{2} \sum _{i=1}^l (y^{(i)} - \transpose{\theta}x^{(i)})^2 + \lambda ||w||^2 \]
Using the direct approach (normal equation) we obtain 
\[ \theta = (\transpose{X}X + \lambda I)^{-1} \transpose{X} y \]
If we compute the gradient descent of the lost function $J(\theta)$ we obtain 
\begin{align*}
	\myDeriv{J(\theta)}{\theta_j} & = \sum _{i=1}^n (y^{(i)} - \transpose{\theta}x^{(i)}) x^{(i)}_j + \myDeriv{\lambda \sum \theta_j^2}{\theta_j} \\
	                              & = \sum _{i=1}^n (y^{(i)} - \transpose{\theta}x^{(i)}) x^{(i)}_j + 2 \lambda \theta_j \\
\end{align*} 
So the update rule for $\theta_j$ is obtained by 
\[ \theta_j = \theta_j - \eta \sum _{i=1}^n (y^{(i)} - \transpose{\theta}x^{(i)}) x^{(i)}_j - 2 \lambda \theta_j \]
The penalty term penalizes high value of the weights $\theta_i$ and tends to drive all the weights to the smaller values, and also it implements a control of the model complexity, where
leads to a model with less VC-dim, but we have to be aware that we have to have an amount of lambda that will not cause underfitting or overfitting.

We have $3$ types of regularization, that will only differ on which type of norm we are using:
\begin{description}
	\item [Ridge Regression: ] it uses the euclidean norm $|| ||_2$
	\item [Lasso Regression: ] it uses the one-norm $|| ||_1$
	\item [Elastic Nets:] use both euclidean and one-norm.
\end{description}
L$2$ norm penalizes the square value of the weight and tends to drive all weights to smaller values, instead L$1$ norm penalizes the absolute value of the weights and tends to drive some 
weights to exactly zero, but it introduce a non differenciable loss function.

\section{Locally Weighted Regression}
Learning algorithms can be divided in two different categories:
\begin{description}
    \item [Parametrics:] fit fixed set of parameters $\theta$ to data
    \item [Non parametrics:] you have to keep an amount of data/parameters, that grows with size of data
\end{description}
\emph{Locally Weight Regression} is our first example of non parametric learning algorithm, where to predict
from a point $x_i$ you have to watch their neighbors.

The objective is to fit $\theta$ to minimize the objective function
\[ \sum _{i = 1}^n w^{(i)}(y^{(i)} - \transpose{\theta}x^{(i)})^2 \]
where $w^{(i)}$ is a weight function and a common choice of weight function is the following
\[ w^{(i)} = exp \left(\frac{-(x^{(i)} - \hat{x})^2}{2 \tau ^ 2}\right) \]
Note that the weights depend on the particular point $x$ at which we’re trying to evaluate $x$ so
if $|x^{(i)} - \hat{x}|$ is small the weight is close to 1, instead if $|x^{(i)} - \hat{x}|$ is 
large the weight is close to $0$.

Hence, $\theta$ is chosen giving a much higher “weight” to the (errors on) training examples close
to the query point $x$ and note also that while the formula for the weights takes a form that is 
cosmetically similar to the density of a Gaussian distribution, the $w(i)$’s do not directly have anything
to do with Gaussians, and in particular the $w(i)$ are not random variables, normally distributed or otherwise.

The parameter $\tau$ controls how quickly the weight of a training example falls off with distance of
its $x^{(i)}$ from the query point $x$ and it is called the \emph{bandwidth} parameter, which choice of value
is important to avoid overfitting and underfitting.

It is commonly used when we have a lot of data and we don't want to think about which features to use.

\section{K-means}
A natural way to classify a new point is to have a look at its neighbors and take an average 
\[ avg_k(x) = \frac{1}{k} \sum _{x^{(i)} \in N_k(x)} y^{(i)} \]
where $N_k(x)$ is a neighborhood of $x$ that contains exactly $k$ neighbors (closest patterns according to distance function $d$).

If there is a clear dominance of one of the classes in the neighborhood of an observation $x$, then it is likely that the observation itself would belong to that class, too, 
thus the classification rule is the majority voting amongthe members of $N_k(x)$ and as before, we have 
\[ h(x) = \begin{cases}
		1 \quad \text{if } avg_K(x) \geq 0.5 \\
		0 \quad \text{otherwise} \\
	  \end{cases} \]
In case of regression task we use directly the average mean over $k$ nearing neighbors and in case of multiclass $k$-nn return the class most common amongst its $k$ nearest neighbors.

The k-means approach is a complete different from linear models introduced in this chapter, infact we went from a rigid model (linear regression), with low variance, to a very flexible model ($k$-nn),
with high variance and also the inductive bias of $k$-nn approach is the assumption of distance, that will tell us which are the most similar examples.

With $k$-nn we have to be aware when we change scale of value, because it can yield to a change on the $k$ most similar values, and also that $k$-nn approach follow a lazy approach,
where we don't create a model but we keep all training data and that causes a huge computation cost during prediction phase.

A last but not least limitation of $k$-nn is that suffer from curve of dimensionality, due the difficult to find near element in high dimensions and also that irrelevant features can
misleading our prediction.
